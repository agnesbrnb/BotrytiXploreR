---
title: "Projet AnaStat"
output: html_document
---

## Chargement des données et construction des jeux d'apprentissage et de test

Les données représentent 184 femmes qui ont été touchées par le cancer du sein ainsi que leur taux d'expression pour 4654 gènes. Nous disposons également d'une variable "label" indiquant la rechute ou non pour chaque femme. L'objectif ici est de déterminer la meilleure méthode d'apprentissage statistique pour prédire la rechute du cancer du sein. Tout d'abord les données sont chargées, mises en forme et divisées en 2 jeux de données : un d'apprentissage et un de test. 
Nous avons donc 184 observations pour 4654 covariables et une variable à expliquer. Nous avons donc beaucoup plus de paramètres que d'observations ce qui complique la construction d'un bon modèle. 

```{r}
setwd("/Volumes/LACIE SHARE/Disque_Agnes/Fac/M2/Ana-Stat")
train <- t(read.table("xtrain.txt", sep="\t", as.is = NA))

data = numeric()
for (i in 2:nrow(train)){
  data <- rbind(data, as.numeric(train[i,]))
}
data <- cbind(data, read.table("ytrain.txt"))
colnames(data) <- c(train[1,], "label")

data <- within(data, {
  label = factor(label, labels = c("-1","1"))
})

cat("Dimension des données :",dim(data), "\n")
str(data)
table(data$label)
```

## Régression logistique multiple

La méthode de régression logistique multiple vise à sélectionner les paramètres en testant l'influence de chaque covariable sur le modèle. Si l'absence de la covariable réduit la qualité du modèle alors sa présence est importante. Ainsi, le meilleur modèle est sélectionné. 
```{r}
data.glm <- glm(label~., data=data, family = binomial)
summary(data.glm)
```

On observe que l'algorithme ne converge pas sur les données surement du au fait du grand nombre de covariables (4654) par rapport au nombre d'observations (184). En effet, la méthode glm ne permet d'estimer que n-1 paramètres où n est le nombre d'observations. Ce modèle ne correspond donc pas aux données. 

## Arbre CART

La méthode CART vise à construire un arbre de décision à partir des covariables. A chaque noeud construit est associé une condition dépendant des covariables. On construit l'arbre maximal à partir des données d'apprentissage puis on réalise une étape d'élagage pour réduire le nombre de feuilles. Cependant, on observe que l'arbre maximal possède 17 feuilles ce qui est peu. On utilise les informations sur l'élagage optimale obtenues avec la fonction rpart pour effectuer l'élagage de l'arbre maximal. On obtient alors un arbre à 7 feuilles.
```{r}
error.cart <- numeric() ; error.cartprune <- numeric()
library(rpart)
for(i in c(2018,1234,2017,1996,1107)) {
  set.seed(i)
  
  test <- sample(1:nrow(data),round(nrow(data)/3))
  train <- (1:nrow(data))[-test]
  train <- data[train,]
  test <- data[test,]

  t.max = rpart(label~., data=train, control=rpart.control(cp = 0))
  t.prune = prune(t.max, cp = t.max$cptable[which.min(t.max$cptable[,4]), 1])
  
  # cat("Arbre maximal\n")
  # print(t.max)
  # plot(t.max)
  # text(t.max, cex=1)
  # 
  # cat("\nArbre après élagage\n")
  # print(t.prune)
  # plot(t.prune)
  # text(t.prune, cex=1)
  
  pred_t = predict(t.max, newdata = test, type="class")
  pred_tprune = predict(t.prune, newdata = test, type="class")
  
  error.cart <- c(error.cart ,sum(test$label != pred_t)/length(test$label))
  error.cartprune <- c(error.cartprune, sum(test$label != pred_tprune)/length(test$label) )
}

cat("Erreur de classification moyenne pour l'arbre maximal :", mean(error.cart))
cat("Erreur de classification moyenne pour l'arbre élagué :", mean(error.cartprune))
```

## Forêt aléatoire

(Décrire randomforest) On réalise la méthode de classification random forest pour expliquer les données y par les covariables. On observe ici un taux d'erreur très important. En effet l'OOB, estimation du taux de mal classés sur l'ensemble des données, est de 39,02% et l'erreur de classification calculée est de 36%. Cette méthode a une erreur plus faible que l'arbre CART mais si elle reste élevée. Ce résultat était attendu puisque la méthode randomForest est efficace dans le traitement de modèle avec un grand nombre de paramètres.

```{r}
error.rf <- numeric()
library(glmnet)
library(randomForest)

for(i in c(2018,1234,2017,1996,1107)) {
  set.seed(i)
  
  test <- sample(1:nrow(data),round(nrow(data)/3))
  train <- (1:nrow(data))[-test]
  train <- data[train,]
  test <- data[test,]
  
  data.rf = randomForest(label~., data=train)
  data.rf
  
  pred.rf <- predict(data.rf, newdata=test)
  error.rf <- c(mean.rf, mean(test$label != pred.rf))
}
mean(error.rf)
```




```{r}
library(randomForest)
  test <- sample(1:nrow(data),round(nrow(data)/3))
  train <- (1:nrow(data))[-test]
  train <- data[train,]
  test <- data[test,]
  data.rf = randomForest(label~., data=train, mtry=68, ntree=2000)
  plot(data.rf)
    
Y <- train$label
tunerf <- tuneRF(train[,-ncol(train)],Y, ntreeTry = 500)
  tunerf
  # pred.rf <- predict(data.rf, newdata=test)
  # error.rf <- c(mean.rf, mean(test$label != pred.rf))

```





## Méthode de bagging

Il s'agit d'une méthode similaire au random forest mais on utilise toutes les covariables pour la construction de chaque noeud et pas un échantillon. (à vérifier)
```{r}
error.bag <- numeric()
for(i in c(2018,1234,2017,1996,1107)) {
  set.seed(i)
  
  test <- sample(1:nrow(data),round(nrow(data)/3))
  train <- (1:nrow(data))[-test]
  train <- data[train,]
  test <- data[test,]
  
  fit.bag <- randomForest(label~., data = train, mtry=ncol(data)-1)
  fit.bag
  
  pred.bag <- predict(fit.bag, newdata=test)
  error.bag <- c(error.bag, mean(test$label != pred.bag))
}

mean(error.bag)
```


## Méthode LASSO

LASSO est une méthode de sélection de variables permettant de réduire les modèles avec de nombreuses covariables. 
```{r}
error.lasso <- numeric() ; nbvar <- c()
library(glmnet)

for(i in c(2018,1234,2017,1996,1107)) {
  set.seed(i)
  
  test <- sample(1:nrow(data),round(nrow(data)/3))
  train <- (1:nrow(data))[-test]
  train <- data[train,]
  test <- data[test,]
  
  X <- model.matrix(label~., data)[,-(ncol(data))]
  Y <- data$label
  
  fit.lasso = glmnet(X,Y,family = "binomial")
  cv.out = cv.glmnet(X,Y, family = "binomial", lambda = seq(exp(-6),1,length = 500))
  cv.out.zoom = cv.glmnet(X,Y, family = "binomial", lambda = seq(exp(-6),exp(-2.5),length = 500))
  
  par(mfrow=c(1,2))
  plot(cv.out)
  plot(cv.out.zoom)
  
  min = cv.out.zoom$lambda.min
  
  Dpred.lasso = predict(fit.lasso, type="coefficients", s=min)
  beta = predict(fit.lasso, type="nonzero", s=min)   # type 'nonzero' retourne les indices des coefficients non nuls
  
  nbvar <- c(nbvar, length(beta[,])) # La longueur de beta correspond donc a la dimension du modèle séléctionné
  
  select = which(coef(cv.out.zoom)!=0)
  data.glm <- glm(label~., data=train[,c(select,ncol(train))], family = binomial)
  summary(data.glm)
  
  proba.glm = predict(data.glm, newdata=test[,c(select,ncol(test))], type="response")
  pred.glm <- rep("-1", length(test$label))
  pred.glm[proba.glm > 1/2] = "1"
  
  
  error.lasso <- c(error.lasso, mean(test$label != pred.glm))
}
mean(error.lasso)

```



## Méthode SVM

(Décrire SVM)
```{r}
library(e1071)
error.lasso <- numeric()

for(i in c(2018,1234,2017,1996,1107)) {
  set.seed(i)
  
  test <- sample(1:nrow(data),round(nrow(data)/3))
  train <- (1:nrow(data))[-test]
  train <- data[train,]
  test <- data[test,]

  tune.out = tune(svm, label~., data = train, kernel = "radial",
                  ranges = list(
                    cost = c(1,100,1000),
                    gamma = c(0.1,1,3)
                  ))
  summary(tune.out)
  
  fit.svm = svm(label~., train, kernel="radial", probability=TRUE, cost=tune.out$best.parameters$cost, gamma=tune.out$best.parameters$gamma)
  
  cat("Erreur de classifaction :",mean(test$label != predict(fit.svm,newdata = test)))
}

```

On peut tracer la courbe ROC afin de visualiser l'erreur d'apprentissage
```{r}
set.seed(2018)
  
  test <- sample(1:nrow(data),round(nrow(data)/3))
  train <- (1:nrow(data))[-test]
  train <- data[train,]
  test <- data[test,]

  tune.out = tune(svm, label~., data = train, kernel = "radial",
                  ranges = list(
                    cost = c(0.1,10),
                    gamma = c(0.1,1)
                  ))
  summary(tune.out)
  
  fit.svm = svm(label~., train, kernel="radial", probability=TRUE, cost=tune.out$best.parameters$cost, gamma=tune.out$best.parameters$gamma)
  
  cat("Erreur de classifaction :",mean(test$label != predict(fit.svm,newdata = test)))
```

